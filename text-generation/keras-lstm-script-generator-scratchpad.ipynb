{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.1-text-generation-with-lstm.ipynb\n",
    "\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_text():\n",
    "    office_script_file_url = \"https://raw.githubusercontent.com/Pradhyo/the-office-us-tv-show/master/the-office-all-episodes.txt\"\n",
    "    path = keras.utils.get_file('script.txt', origin=office_script_file_url)\n",
    "    text = open(path).read().lower()\n",
    "    return text\n",
    "\n",
    "text = get_text()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6cca9ff2c1c9018c33a0020a94f27476ae204ca6"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "char_counts = Counter()\n",
    "for c in text:\n",
    "    char_counts[c] += 1\n",
    "    \n",
    "pprint(char_counts.most_common())\n",
    "pprint(len(char_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e08e65fa058c0fa2c38323d0a37307a1498e1ed4"
   },
   "outputs": [],
   "source": [
    "def sample_strings(char, string_length=20, num_samples=5):\n",
    "    sample = 0\n",
    "    samples = []\n",
    "    for i, c in enumerate(text):\n",
    "        if i < string_length:\n",
    "            continue\n",
    "        if char == c:\n",
    "            samples.append(text[int(i-string_length/2):int(i+string_length/2)])\n",
    "            sample += 1\n",
    "            if sample == num_samples:\n",
    "                break\n",
    "    return samples\n",
    "\n",
    "for c in char_counts:\n",
    "    print(f\"{c}: {sample_strings(c)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4333856dab3fec3329891b9ab2706a4462c2905"
   },
   "outputs": [],
   "source": [
    "for c in char_counts:\n",
    "    if not c.isalnum():\n",
    "        print(f\"{c}: {sample_strings(c, 40)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7688a90be78aa33bc17f89345a4198b793fea7d2"
   },
   "source": [
    "Looking at the above text, some of the characters like `\\n`appear in between words but some of them like `'` appear as part of the word. \n",
    "I am going to leave the ones within words as is but consider the others as separate words so the model doesn't consider *jim* in`\\njim` different from  just`jim`. I am also going to consider all numbers the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a76b4c3763cd551efebe9c322cff5ae5a8ce1a5"
   },
   "outputs": [],
   "source": [
    "consider_words = ''.join(c for c in char_counts if not c.isalnum())\n",
    "print(consider_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eb6957bf5a14c5fadd43b7f1d7838c26ce7de58"
   },
   "source": [
    "Looking at the symbols more closely, it doesn't look like there are a lot of symbols that appear within the words so I am just going to consider all of them separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b5bc6871db2f5b3f7276df5106f3fa1eabcad53e"
   },
   "outputs": [],
   "source": [
    "numbers = '0123456789'\n",
    "def replace_numbers(text):\n",
    "    for n in numbers:\n",
    "        text = text.replace(n, \"0\")\n",
    "    return text\n",
    "\n",
    "text = replace_numbers(text)\n",
    "consider_words += '0'\n",
    "print(consider_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ee1e24211bc0a4ea2a347440c7bbbfe71847cf85"
   },
   "outputs": [],
   "source": [
    "def split_into_words(text, consider_words):\n",
    "    # Split text into words - characters above are also considered words\n",
    "    text = text.replace(' ', ' | ') # pick a char not in the above list\n",
    "    text = text.replace('\\n', ' | ') # pick a char not in the above list\n",
    "\n",
    "    for char in consider_words:\n",
    "        text = text.replace(char, f\" {char} \") # to split on spaces to get char\n",
    "\n",
    "    words_with_pipe = text.split()\n",
    "    words = [word if word != '|' else ' ' for word in words_with_pipe]\n",
    "    return words\n",
    "\n",
    "words = split_into_words(text, consider_words)\n",
    "print(words[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e685bba37980e9418b4ed025863a5038b9996dc6"
   },
   "outputs": [],
   "source": [
    "# Length of extracted word sequences\n",
    "maxlen = 20\n",
    "\n",
    "# We sample a new sequence every `step` words\n",
    "step = 3\n",
    "\n",
    "unique_words = []\n",
    "word_indices = {}\n",
    "\n",
    "def setup_x_y(words, maxlen, step):\n",
    "    global unique_words\n",
    "    global word_indices\n",
    "    # This holds our extracted sequences\n",
    "    sentences = []\n",
    "\n",
    "    # This holds the targets (the follow-up characters)\n",
    "    next_words = []\n",
    "\n",
    "    for i in range(0, len(words) - maxlen, step):\n",
    "        sentences.append(words[i: i + maxlen])\n",
    "        next_words.append(words[i + maxlen])\n",
    "    print('Number of sequences:', len(sentences))\n",
    "\n",
    "    # List of unique characters in the corpus\n",
    "    unique_words = sorted(list(set(words)))\n",
    "    print('Unique words:', len(unique_words))\n",
    "    # Dictionary mapping unique characters to their index in `unique_words`\n",
    "    word_indices = dict((word, unique_words.index(word)) for word in unique_words)\n",
    "\n",
    "    # Next, one-hot encode the characters into binary arrays.\n",
    "    print('Vectorization...')\n",
    "    x = np.zeros((len(sentences), maxlen, len(unique_words)), dtype=np.bool)\n",
    "    y = np.zeros((len(sentences), len(unique_words)), dtype=np.bool)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            x[i, t, word_indices[word]] = 1\n",
    "        y[i, word_indices[next_words[i]]] = 1\n",
    "    return x, y\n",
    "\n",
    "# Commenting out since this is causing Memory Error\n",
    "# x, y = setup_x_y(words, maxlen, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bdd5e5ca1d8320501e456e32c138d4d62f0e56d0"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "text = get_text()\n",
    "\n",
    "selected_actor = \"phyllis\"\n",
    "\n",
    "def get_selected_lines(text, selected_actor):\n",
    "    lines = text.split(\"\\n\")\n",
    "    return \"\\n\".join(line for line in lines if line.startswith(f\"{selected_actor}:\"))\n",
    "\n",
    "text = get_selected_lines(text, selected_actor)\n",
    "print(text[:2000])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa029ca2ba6a0f3290fc36d47e7ab1e1a6651eca"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "text = replace_numbers(text)\n",
    "words = split_into_words(text, consider_words)\n",
    "x, y = setup_x_y(words, maxlen, step)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f8b2beb0c6c202cbe948122d395c70118e6702ed"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(unique_words))))\n",
    "model.add(layers.Dense(len(unique_words), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "993f41e2c1883cb2155ee6cf2a82fb615a6329d9"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7d6c0d554e9e28a5027051cc78140e891c32416"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f0e6abf7a2cc2b555c9c6663a1135a9a4bf33f2"
   },
   "outputs": [],
   "source": [
    "\"\"\"import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = words[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + ''.join(generated_text) + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(''.join(generated_text))\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(unique_words)))\n",
    "            for t, word in enumerate(generated_text):\n",
    "                sampled[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = unique_words[next_index]\n",
    "\n",
    "            generated_text.append(next_word)\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_word)\n",
    "            sys.stdout.flush()\n",
    "        print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e654fc5ddc22cdf00c087fce4046b7f5f9bc2cbf"
   },
   "outputs": [],
   "source": [
    "text = get_text()\n",
    "text = replace_numbers(text)\n",
    "words = split_into_words(text, consider_words)\n",
    "words_counter = Counter(words)\n",
    "print(len(words_counter))\n",
    "print(words_counter.most_common(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2ef0fe6563445ec8b88f15cde154b9079e5ebd9"
   },
   "outputs": [],
   "source": [
    "top_words = []\n",
    "for word, count in words_counter.most_common(2000):\n",
    "        top_words.append(word)\n",
    "\n",
    "print(len(top_words))\n",
    "\n",
    "def get_lines_with_words(top_words):\n",
    "    selected_lines = []\n",
    "    text = get_text()\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = replace_numbers(line)\n",
    "        words_in_line = split_into_words(line, consider_words)\n",
    "        excluded_words = 0\n",
    "        for word_in_line in words_in_line:\n",
    "            if word_in_line not in top_words:\n",
    "                excluded_words += 1\n",
    "                break\n",
    "        if not excluded_words:\n",
    "            selected_lines.append(line)\n",
    "    return selected_lines\n",
    "                \n",
    "                \n",
    "selected_lines = get_lines_with_words(top_words)\n",
    "print(len(selected_lines))\n",
    "print(selected_lines[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7eeba715907f419ec26128e791a48d7f320704ab"
   },
   "outputs": [],
   "source": [
    "selected_text = \"\\n\".join(selected_lines)\n",
    "\n",
    "selected_text = replace_numbers(selected_text)\n",
    "selected_words = split_into_words(selected_text, consider_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d06b774ca35619e755a177c99097934d971d8850"
   },
   "outputs": [],
   "source": [
    "x, y = setup_x_y(selected_words, maxlen, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87dbeb4e362f4b2fbda0a81e13398550c53bd8cd"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(unique_words))))\n",
    "model.add(layers.Dense(len(unique_words), activation='softmax'))\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "with open(\"generated_script.txt\", \"wt\") as f:\n",
    "    f.write(\"Creating file to write output\\n\\n\")\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    with open(\"generated_script.txt\", \"at\") as f:\n",
    "        f.write(f'\\n\\nepoch {epoch}\\n\\n')\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(selected_text) - maxlen - 1)\n",
    "    generated_text = selected_words[start_index: start_index + maxlen]\n",
    "\n",
    "    with open(\"generated_script.txt\", \"at\") as f:\n",
    "        f.write('--- Generating with seed: \"' + ''.join(generated_text) + '\"\\n')\n",
    "\n",
    "    with open(\"generated_script.txt\", \"at\") as f:        \n",
    "        for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "            f.write('\\n--- temperature: ' + str(temperature) + \"\\n\")\n",
    "            f.write(''.join(generated_text))\n",
    "\n",
    "            for i in range(200):\n",
    "                sampled = np.zeros((1, maxlen, len(unique_words)))\n",
    "                for t, word in enumerate(generated_text):\n",
    "                    sampled[0, t, word_indices[word]] = 1.\n",
    "\n",
    "                preds = model.predict(sampled, verbose=0)[0]\n",
    "                next_index = sample(preds, temperature)\n",
    "                next_word = unique_words[next_index]\n",
    "\n",
    "                generated_text.append(next_word)\n",
    "                generated_text = generated_text[1:]\n",
    "\n",
    "                f.write(next_word)\n",
    "    model.save(\"top_lines.h5\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
